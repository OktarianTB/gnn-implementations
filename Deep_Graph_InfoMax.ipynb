{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Graph InfoMax.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW29WrOxhilO"
      },
      "source": [
        "# Implementation of Deep Graph InfoMax\n",
        "\n",
        "Based upon:\n",
        "- **Paper:** Deep Graph Infomax (Veličković et al., ICLR 2019) https://arxiv.org/pdf/1809.10341.pdf\n",
        "\n",
        "- **Implementation:** https://github.com/PetarV-/DGI\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syZw3vBuhx9f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pickle as pkl\n",
        "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
        "import sys\n",
        "import networkx as nx"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdf3BtAnh-xs"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FmC56o4_FTS"
      },
      "source": [
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "def load_data(path): \n",
        "    \"\"\"Load data.\"\"\"\n",
        "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "    objects = []\n",
        "    dataset_str = \"cora\"\n",
        "    for i in range(len(names)):\n",
        "        with open(\"{}/ind.{}.{}\".format(path, dataset_str, names[i]), 'rb') as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding='latin1'))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\"{}/ind.{}.test.index\".format(path, dataset_str))\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    features = sp.vstack((allx, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "\n",
        "    labels = np.vstack((ally, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "    idx_test = test_idx_range.tolist()\n",
        "    idx_train = range(len(y))\n",
        "    idx_val = range(len(y), len(y)+500)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "def sparse_to_tuple(sparse_mx, insert_batch=False):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    \"\"\"Set insert_batch=True if you want to insert a batch dimension.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        if insert_batch:\n",
        "            coords = np.vstack((np.zeros(mx.row.shape[0]), mx.row, mx.col)).transpose()\n",
        "            values = mx.data\n",
        "            shape = (1,) + mx.shape\n",
        "        else:\n",
        "            coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "            values = mx.data\n",
        "            shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "def standardize_data(f, train_mask):\n",
        "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
        "    # standardize data\n",
        "    f = f.todense()\n",
        "    mu = f[train_mask == True, :].mean(axis=0)\n",
        "    sigma = f[train_mask == True, :].std(axis=0)\n",
        "    f = f[:, np.squeeze(np.array(sigma > 0))]\n",
        "    mu = f[train_mask == True, :].mean(axis=0)\n",
        "    sigma = f[train_mask == True, :].std(axis=0)\n",
        "    f = (f - mu) / sigma\n",
        "    return f\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return features.todense(), sparse_to_tuple(features)\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itYfDS6jiEJ9"
      },
      "source": [
        "## Deep Graph InfoMax Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDG-A63Sicpl"
      },
      "source": [
        "### GCN Model\n",
        "\n",
        "We define an encoder $ \\mathcal{E}:\\mathcal{R}^{N\\times F}\\times\\mathcal{R}^{N\\times N}\\rightarrow \\mathcal{R}^{N\\times F'}$ such that $ \\mathcal{E}(X,A)=H=\\{\\vec{h}_1, \\vec{h}_2,.., \\vec{h}_N\\}$ represents high-level representations $\\vec{h}_i\\in \\mathcal{R}^{F'} $ for each node $i$.\n",
        "\n",
        "\n",
        "Here, we use the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)) defined as: $ \\mathcal{E}(X,A)=\\sigma(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}X\\Theta)\n",
        "$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nYyb3aphyVO"
      },
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_ft, out_ft, bias=True):\n",
        "        super(GCN, self).__init__()\n",
        "        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n",
        "        self.act = nn.PReLU()\n",
        "        \n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n",
        "            self.bias.data.fill_(0.0)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    # Shape of seq: (batch, nodes, features)\n",
        "    def forward(self, seq, adj, sparse=False):\n",
        "        seq_fts = self.fc(seq)\n",
        "        if sparse:\n",
        "            out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)\n",
        "        else:\n",
        "            out = torch.bmm(adj, seq_fts)\n",
        "        if self.bias is not None:\n",
        "            out += self.bias\n",
        "        \n",
        "        return self.act(out)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgzoQug9l_UE"
      },
      "source": [
        "### Readout Function\n",
        "We leverage a readout function $\\mathcal{R}:\\mathbb{R} ^{N\\times F}\\rightarrow \\mathbb{R}^F$ and use it to summarize the obtained patch representations into a graph-level representation, i.e. $\\vec{s}=\\mathcal{R}(\\mathcal{E}(X,A))$.\n",
        "\n",
        "We use a simple averaging of all the nodes' features: $ \\mathcal{R}(H)=\\sigma(\\frac{1}{N}\\sum_{i=1}^N\\vec{h}_i) $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T222DLqjiRWA"
      },
      "source": [
        "class AvgReadout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AvgReadout, self).__init__()\n",
        "\n",
        "    def forward(self, seq, msk):\n",
        "        if msk is None:\n",
        "            return torch.mean(seq, 1)\n",
        "        else:\n",
        "            msk = torch.unsqueeze(msk, -1)\n",
        "            return torch.sum(seq * msk, 1) / torch.sum(msk)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB5qVS0amkGz"
      },
      "source": [
        "### Discriminator Function\n",
        "\n",
        "As a proxy for maximizing the local mutual information, we employ a discriminator $\\mathcal{D}:\\mathbb{R}^F\\times \\mathbb{R}^F\\rightarrow \\mathbb{R}$ such that $\\mathcal{D}(\\vec{h}_i,\\vec{s})$ represents the probability scores assigned to this patch-summary pair.\n",
        "\n",
        "Negative samples for $\\mathcal{D}$ are provided by pairing the summary $\\vec{s}$ from $(X,A)$ with patch representations $\\vec{\\tilde{h}}_j$ of an alternative graph $(\\tilde{X},\\tilde{A})$:\n",
        "\n",
        "- In a multi-graph setting, such graphs may be obtained as other elements of a training set\n",
        "- For a single graph, an explicit corruption function is required to obtain a negative example from the original graph\n",
        "\n",
        "Here, we score summary-patch representation pairs by applying a simple bilinear scoring function with W as a learnable scoring matrix: $D(\\vec{h}_i,\\vec{s})=\\sigma(\\vec{h}_i^TW\\vec{s})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFOZDA2XiRYc"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_h):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Bilinear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
        "        c_x = torch.unsqueeze(c, 1)\n",
        "        c_x = c_x.expand_as(h_pl)\n",
        "\n",
        "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
        "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
        "\n",
        "        if s_bias1 is not None:\n",
        "            sc_1 += s_bias1\n",
        "        if s_bias2 is not None:\n",
        "            sc_2 += s_bias2\n",
        "\n",
        "        logits = torch.cat((sc_1, sc_2), 1)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x34Jx830nou3"
      },
      "source": [
        "## Deep Graph InfoMax Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAsZLHwmpb40"
      },
      "source": [
        "### DGI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxaLknztiRaw"
      },
      "source": [
        "class DGI(nn.Module):\n",
        "    def __init__(self, n_in, n_h):\n",
        "        super(DGI, self).__init__()\n",
        "        self.gcn = GCN(n_in, n_h)\n",
        "        self.read = AvgReadout()\n",
        "\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "        self.disc = Discriminator(n_h)\n",
        "\n",
        "    def forward(self, seq1, seq2, adj, sparse, msk, samp_bias1, samp_bias2):\n",
        "        h_1 = self.gcn(seq1, adj, sparse)\n",
        "\n",
        "        c = self.read(h_1, msk)\n",
        "        c = self.sigm(c)\n",
        "\n",
        "        h_2 = self.gcn(seq2, adj, sparse)\n",
        "\n",
        "        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    # Detach the return variables\n",
        "    def embed(self, seq, adj, sparse, msk):\n",
        "        h_1 = self.gcn(seq, adj, sparse)\n",
        "        c = self.read(h_1, msk)\n",
        "\n",
        "        return h_1.detach(), c.detach()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnwpR1ZhsgqA"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX8f1FA2iRdI"
      },
      "source": [
        "class LogReg(nn.Module):\n",
        "    def __init__(self, ft_in, nb_classes):\n",
        "        super(LogReg, self).__init__()\n",
        "        self.fc = nn.Linear(ft_in, nb_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        ret = self.fc(seq)\n",
        "        return ret\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV-Sa8ETz3FF"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U48HjWiez6h0",
        "outputId": "e3f61dd1-4bc6-46e3-869f-368860244a45"
      },
      "source": [
        "# Load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"./drive/MyDrive/Colab Notebooks/Representation Learning for GNNs/data/cora/\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCAqY8aQz6pp"
      },
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = load_data(path)\n",
        "features, _ = preprocess_features(features)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0E2x7T7s0j3"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NskxgS1hyX2"
      },
      "source": [
        "args = {\n",
        "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \"epochs\" : 10000,\n",
        "    \"patience\": 20,\n",
        "    \"lr\" : 0.001,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"dropout\": 0.0,\n",
        "    \"hidden\" : 512,\n",
        "    \"batch_size\": 1,\n",
        "    \"sparse\": True\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SdRBy2ahyaV"
      },
      "source": [
        "nb_nodes = features.shape[0]\n",
        "ft_size = features.shape[1]\n",
        "nb_classes = labels.shape[1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5K57fIhEy0y"
      },
      "source": [
        "adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "if args[\"sparse\"]:\n",
        "    sp_adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "else:\n",
        "    adj = (adj + sp.eye(adj.shape[0])).todense()\n",
        "\n",
        "features = torch.FloatTensor(features[np.newaxis])\n",
        "if not args[\"sparse\"]:\n",
        "    adj = torch.FloatTensor(adj[np.newaxis])\n",
        "labels = torch.FloatTensor(labels[np.newaxis])\n",
        "idx_train = torch.LongTensor(idx_train)\n",
        "idx_val = torch.LongTensor(idx_val)\n",
        "idx_test = torch.LongTensor(idx_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXnx27gSE8fo"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('Using CUDA')\n",
        "    features = features.cuda()\n",
        "    if args[\"sparse\"]:\n",
        "        sp_adj = sp_adj.cuda()\n",
        "    else:\n",
        "        adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89n-jdAruOI_"
      },
      "source": [
        "b_xent = nn.BCEWithLogitsLoss()\n",
        "xent = nn.CrossEntropyLoss()\n",
        "cnt_wait = 0\n",
        "best = 1e9\n",
        "best_t = 0\n",
        "batch_size = 1"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8ZLlFuStdrt"
      },
      "source": [
        "model = DGI(ft_size, args[\"hidden\"]).to(args[\"device\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBB52lVzuYWX",
        "outputId": "423b72d3-515d-4953-bda3-09b564205597"
      },
      "source": [
        "for epoch in range(args[\"epochs\"] + 1):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  idx = np.random.permutation(nb_nodes)\n",
        "  shuf_fts = features[:, idx, :]\n",
        "\n",
        "  lbl_1 = torch.ones(batch_size, nb_nodes)\n",
        "  lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
        "  lbl = torch.cat((lbl_1, lbl_2), 1)\n",
        "\n",
        "  shuf_fts = shuf_fts.to(args[\"device\"])\n",
        "  lbl = lbl.to(args[\"device\"])\n",
        "  \n",
        "  logits = model(features, shuf_fts, sp_adj if args[\"sparse\"] else adj, args[\"sparse\"], None, None, None) \n",
        "  loss = b_xent(logits, lbl)\n",
        "  print('Loss:', loss)\n",
        "\n",
        "  if loss < best:\n",
        "      best = loss\n",
        "      best_t = epoch\n",
        "      cnt_wait = 0\n",
        "      torch.save(model.state_dict(), 'best_dgi.pkl')\n",
        "  else:\n",
        "      cnt_wait += 1\n",
        "\n",
        "  if cnt_wait == args[\"patience\"]:\n",
        "      print('Early stopping!')\n",
        "      break\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6927, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6874, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6860, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6818, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6759, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6721, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6653, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6585, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6506, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6417, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6342, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6227, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.6131, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.5993, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.5920, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.5744, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.5636, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.5507, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.5341, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.5235, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.5056, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.4989, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.4848, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.4657, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.4518, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.4451, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.4248, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.4135, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3976, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3911, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3755, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3626, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3570, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3416, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3411, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3209, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3075, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.3064, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2911, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2848, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2831, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2652, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2626, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2550, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2458, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2344, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2360, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2298, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2266, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2261, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2134, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2125, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1975, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2104, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2081, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.2170, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1916, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1993, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1914, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1833, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1850, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1864, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1709, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1746, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1787, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1720, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1668, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1552, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1656, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1561, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1499, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1541, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1456, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1502, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1501, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1538, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1535, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1465, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1564, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1427, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1443, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1402, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1269, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1382, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1322, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1228, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1339, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1360, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1193, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1338, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1325, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1246, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1168, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1302, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1224, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1281, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1230, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1136, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1234, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1102, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1150, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1119, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1241, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1102, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1087, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1082, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1057, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1238, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1306, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1125, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0976, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0997, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0985, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1090, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1008, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1048, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1083, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0942, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0990, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0935, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1076, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1064, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1047, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0891, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0924, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1004, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0905, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0901, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0966, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.1030, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0851, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0993, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0924, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0982, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0907, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0933, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0818, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0858, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0829, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0875, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0928, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0797, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0915, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0907, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0755, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0837, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0771, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0826, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0899, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0780, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0891, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0801, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0904, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0824, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0814, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0858, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0696, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0818, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0782, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0737, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0748, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0700, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0774, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0785, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0756, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0666, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0754, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0767, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0722, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0687, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0708, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0769, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0690, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0747, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0820, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0805, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0694, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0756, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0653, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0718, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0740, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0712, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0589, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0764, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0600, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0730, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0613, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0716, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0690, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0674, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0621, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0720, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0743, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0591, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0616, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0633, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0871, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0582, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0700, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0554, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0716, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0706, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0624, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0610, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0687, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0660, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0702, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0685, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0623, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0747, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0610, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0647, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0559, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0592, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0674, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0585, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0685, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0611, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0647, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0520, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0655, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0604, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0639, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0551, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0656, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0597, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0637, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0533, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0550, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0501, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0518, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0515, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0514, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0557, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0589, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0550, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0478, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0525, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0465, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0486, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0511, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0510, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0548, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0448, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0510, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0519, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0600, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0432, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0472, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0514, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0514, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0528, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0531, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0551, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0557, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0448, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0499, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0559, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0536, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0423, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0534, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0504, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0555, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0516, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0517, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0553, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0534, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0516, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0510, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0565, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0545, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0429, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0558, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0532, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0555, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0532, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0456, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0498, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0508, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Loss: tensor(0.0572, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Early stopping!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8WYPJUcwHwr"
      },
      "source": [
        "print('Loading {}th epoch'.format(best_t))\n",
        "model.load_state_dict(torch.load('best_dgi.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85qRWEJSwHzH"
      },
      "source": [
        "embeds, _ = model.embed(features, sp_adj if args[\"sparse\"] else adj, args[\"sparse\"], None)\n",
        "train_embs = embeds[0, idx_train]\n",
        "val_embs = embeds[0, idx_val]\n",
        "test_embs = embeds[0, idx_test]\n",
        "\n",
        "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
        "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
        "test_lbls = torch.argmax(labels[0, idx_test], dim=1)\n",
        "\n",
        "tot = torch.zeros(1)\n",
        "\n",
        "accs = []"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCYwbFoSwH1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e152c8f-de1b-48ce-fec7-04bb44d461fe"
      },
      "source": [
        "for index in range(50):\n",
        "    log = LogReg(args[\"hidden\"], nb_classes)\n",
        "    opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
        "\n",
        "    pat_steps = 0\n",
        "    best_acc = torch.zeros(1)\n",
        "    for _ in range(100):\n",
        "        log.train()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        logits = log(train_embs)\n",
        "        loss = xent(logits, train_lbls)\n",
        "        \n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    logits = log(test_embs)\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
        "    accs.append(acc * 100)\n",
        "    print(acc)\n",
        "    tot += acc\n",
        "\n",
        "print('Average accuracy:', tot / 50)\n",
        "\n",
        "accs = torch.stack(accs)\n",
        "print(accs.mean())\n",
        "print(accs.std())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.8240)\n",
            "tensor(0.8190)\n",
            "tensor(0.8240)\n",
            "tensor(0.8220)\n",
            "tensor(0.8220)\n",
            "tensor(0.8210)\n",
            "tensor(0.8250)\n",
            "tensor(0.8210)\n",
            "tensor(0.8230)\n",
            "tensor(0.8240)\n",
            "tensor(0.8260)\n",
            "tensor(0.8240)\n",
            "tensor(0.8230)\n",
            "tensor(0.8250)\n",
            "tensor(0.8240)\n",
            "tensor(0.8240)\n",
            "tensor(0.8220)\n",
            "tensor(0.8210)\n",
            "tensor(0.8240)\n",
            "tensor(0.8240)\n",
            "tensor(0.8230)\n",
            "tensor(0.8240)\n",
            "tensor(0.8250)\n",
            "tensor(0.8240)\n",
            "tensor(0.8210)\n",
            "tensor(0.8230)\n",
            "tensor(0.8260)\n",
            "tensor(0.8230)\n",
            "tensor(0.8240)\n",
            "tensor(0.8210)\n",
            "tensor(0.8220)\n",
            "tensor(0.8230)\n",
            "tensor(0.8270)\n",
            "tensor(0.8240)\n",
            "tensor(0.8240)\n",
            "tensor(0.8200)\n",
            "tensor(0.8240)\n",
            "tensor(0.8230)\n",
            "tensor(0.8220)\n",
            "tensor(0.8240)\n",
            "tensor(0.8240)\n",
            "tensor(0.8230)\n",
            "tensor(0.8200)\n",
            "tensor(0.8240)\n",
            "tensor(0.8250)\n",
            "tensor(0.8230)\n",
            "tensor(0.8210)\n",
            "tensor(0.8240)\n",
            "tensor(0.8230)\n",
            "tensor(0.8240)\n",
            "Average accuracy: tensor([0.8232])\n",
            "tensor(82.3200)\n",
            "tensor(0.1629)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ru5RIXkwH4i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}