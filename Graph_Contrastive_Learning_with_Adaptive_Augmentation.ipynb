{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph Contrastive Learning with Adaptive Augmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22-nH0e9yK_t"
      },
      "source": [
        "# Graph Contrastive Learning with Adaptive Augmentation\n",
        "Based on:\n",
        "- **Paper:** https://arxiv.org/pdf/2010.14945.pdf\n",
        "- **Implementation:** https://github.com/CRIPAC-DIG/GCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul-vupDqyYrJ"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pREuLUZRydId"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import WikiCS\n",
        "from torch_geometric.utils import dropout_adj, degree, to_undirected\n",
        "import torch_geometric.transforms as T\n",
        "from torch.utils.data import random_split\n",
        "from torch_geometric.nn import GCNConv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh7--uDn9Z_6"
      },
      "source": [
        "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device_type)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-ZBbwblyV_D"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjYDL9wSyGoi",
        "outputId": "d506eb3d-21e1-4936-d5d1-945047109374"
      },
      "source": [
        "dataset = WikiCS(root=\"data/WikiCS\", transform=T.NormalizeFeatures())\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "data = data.to(device)\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset: WikiCS():\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 300\n",
            "Number of classes: 10\n",
            "\n",
            "Data(edge_index=[2, 297110], stopping_mask=[11701, 20], test_mask=[11701], train_mask=[11701, 20], val_mask=[11701, 20], x=[11701, 300], y=[11701])\n",
            "===========================================================================================================\n",
            "Number of nodes: 11701\n",
            "Number of edges: 297110\n",
            "Average node degree: 25.39\n",
            "Number of training nodes: 11600\n",
            "Training node label rate: 0.99\n",
            "Contains isolated nodes: True\n",
            "Contains self-loops: True\n",
            "Is undirected: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCGGolkp1hsr"
      },
      "source": [
        "def generate_split(num_samples: int, train_ratio: float, val_ratio: float):\n",
        "    train_len = int(num_samples * train_ratio)\n",
        "    val_len = int(num_samples * val_ratio)\n",
        "    test_len = num_samples - train_len - val_len\n",
        "\n",
        "    train_set, test_set, val_set = random_split(torch.arange(0, num_samples), (train_len, test_len, val_len))\n",
        "\n",
        "    idx_train, idx_test, idx_val = train_set.indices, test_set.indices, val_set.indices\n",
        "    train_mask = torch.zeros((num_samples,)).to(torch.bool)\n",
        "    test_mask = torch.zeros((num_samples,)).to(torch.bool)\n",
        "    val_mask = torch.zeros((num_samples,)).to(torch.bool)\n",
        "\n",
        "    train_mask[idx_train] = True\n",
        "    test_mask[idx_test] = True\n",
        "    val_mask[idx_val] = True\n",
        "\n",
        "    return {\n",
        "            'train': train_mask,\n",
        "            'test': test_mask,\n",
        "            'val': val_mask\n",
        "        }"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvO_USgK-wQ5"
      },
      "source": [
        "split = generate_split(data.num_nodes, train_ratio=0.1, val_ratio=0.1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABk_9lrG_FAM"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewADFfJT-485"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, num_layers: int = 2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.conv = [GCNConv(in_channels, 2 * out_channels).jittable()]\n",
        "        for _ in range(1, num_layers - 1):\n",
        "            self.conv.append(base_model(2 * out_channels, 2 * out_channels))\n",
        "        self.conv.append(GCNConv(2 * out_channels, out_channels))\n",
        "\n",
        "        self.conv = nn.ModuleList(self.conv)\n",
        "        self.activation = nn.PReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n",
        "        for i in range(self.num_layers):\n",
        "                x = self.activation(self.conv[i](x, edge_index))\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fzO6Y67I23B"
      },
      "source": [
        "class GRACE(torch.nn.Module):\n",
        "    def __init__(self, encoder: Encoder, num_hidden: int, num_proj_hidden: int, tau: float = 0.5):\n",
        "        super(GRACE, self).__init__()\n",
        "        self.encoder: Encoder = encoder\n",
        "        self.tau: float = tau\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(num_hidden, num_proj_hidden)\n",
        "        self.fc2 = torch.nn.Linear(num_proj_hidden, num_hidden)\n",
        "\n",
        "        self.num_hidden = num_hidden\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        return self.encoder(x, edge_index)\n",
        "\n",
        "    def projection(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        z = F.elu(self.fc1(z))\n",
        "        return self.fc2(z)\n",
        "\n",
        "    def sim(self, z1: torch.Tensor, z2: torch.Tensor):\n",
        "        z1 = F.normalize(z1)\n",
        "        z2 = F.normalize(z2)\n",
        "        return torch.mm(z1, z2.t())\n",
        "\n",
        "    def semi_loss(self, z1: torch.Tensor, z2: torch.Tensor):\n",
        "        f = lambda x: torch.exp(x / self.tau)\n",
        "        refl_sim = f(self.sim(z1, z1))\n",
        "        between_sim = f(self.sim(z1, z2))\n",
        "\n",
        "        return -torch.log(between_sim.diag() / (refl_sim.sum(1) + between_sim.sum(1) - refl_sim.diag()))\n",
        "\n",
        "    def loss(self, z1: torch.Tensor, z2: torch.Tensor, mean: bool = True):\n",
        "        h1 = self.projection(z1)\n",
        "        h2 = self.projection(z2)\n",
        "\n",
        "        l1 = self.semi_loss(h1, h2)\n",
        "        l2 = self.semi_loss(h2, h1)\n",
        "\n",
        "        ret = (l1 + l2) * 0.5\n",
        "        ret = ret.mean() if mean else ret.sum()\n",
        "\n",
        "        return ret"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzGK89fcI25T"
      },
      "source": [
        "class LogReg(nn.Module):\n",
        "    def __init__(self, ft_in, nb_classes):\n",
        "        super(LogReg, self).__init__()\n",
        "        self.fc = nn.Linear(ft_in, nb_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        ret = self.fc(seq)\n",
        "        return ret"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-9e1UfuUkg_"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwnl7joiUlMq"
      },
      "source": [
        "def degree_drop_weights(edge_index):\n",
        "    edge_index_ = to_undirected(edge_index)\n",
        "    deg = degree(edge_index_[1])\n",
        "    deg_col = deg[edge_index[1]].to(torch.float32)\n",
        "    s_col = torch.log(deg_col)\n",
        "    weights = (s_col.max() - s_col) / (s_col.max() - s_col.mean())\n",
        "\n",
        "    return weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rmu0apaUk6a"
      },
      "source": [
        "def feature_drop_weights_dense(x, node_c):\n",
        "    x = x.abs()\n",
        "    w = x.t() @ node_c\n",
        "    w = w.log()\n",
        "    s = (w.max() - w) / (w.max() - w.mean())\n",
        "\n",
        "    return s"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkj79Ztua4J9"
      },
      "source": [
        "def drop_edge_weighted(edge_index, edge_weights, p: float, threshold: float = 1.):\n",
        "    edge_weights = edge_weights / edge_weights.mean() * p\n",
        "    edge_weights = edge_weights.where(edge_weights < threshold, torch.ones_like(edge_weights) * threshold)\n",
        "    sel_mask = torch.bernoulli(1. - edge_weights).to(torch.bool)\n",
        "\n",
        "    return edge_index[:, sel_mask]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDdERS8WbEzd"
      },
      "source": [
        "def drop_feature(x, drop_prob):\n",
        "    drop_mask = torch.empty((x.size(1),), dtype=torch.float32, device=x.device).uniform_(0, 1) < drop_prob\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0\n",
        "\n",
        "    return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG9gx4mLbP3v"
      },
      "source": [
        "def drop_feature_weighted_2(x, w, p: float, threshold: float = 0.7):\n",
        "    w = w / w.mean() * p\n",
        "    w = w.where(w < threshold, torch.ones_like(w) * threshold)\n",
        "    drop_prob = w\n",
        "\n",
        "    drop_mask = torch.bernoulli(drop_prob).to(torch.bool)\n",
        "\n",
        "    x = x.clone()\n",
        "    x[:, drop_mask] = 0.\n",
        "\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2S44GwEgRRU"
      },
      "source": [
        "class MulticlassEvaluator:\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def _eval(y_true, y_pred):\n",
        "        y_true = y_true.view(-1)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        total = y_true.size(0)\n",
        "        correct = (y_true == y_pred).to(torch.float32).sum()\n",
        "        return (correct / total).item()\n",
        "\n",
        "    def eval(self, res):\n",
        "        return {'acc': self._eval(**res)}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRxuvxBOc4cD"
      },
      "source": [
        "def log_regression(z,\n",
        "                   dataset,\n",
        "                   evaluator,\n",
        "                   num_epochs: int = 5000,\n",
        "                   test_device=None,\n",
        "                   split: str = 'rand:0.1',\n",
        "                   verbose: bool = False,\n",
        "                   preload_split=None):\n",
        "    test_device = z.device if test_device is None else test_device\n",
        "    z = z.detach().to(test_device)\n",
        "    num_hidden = z.size(1)\n",
        "    y = dataset[0].y.view(-1).to(test_device)\n",
        "    num_classes = dataset[0].y.max().item() + 1\n",
        "    classifier = LogReg(num_hidden, num_classes).to(test_device)\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=0.0)\n",
        "\n",
        "    split = {k: v.to(test_device) for k, v in split.items()}\n",
        "    f = nn.LogSoftmax(dim=-1)\n",
        "    nll_loss = nn.NLLLoss()\n",
        "\n",
        "    best_test_acc = 0\n",
        "    best_val_acc = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        classifier.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = classifier(z[split['train']])\n",
        "        loss = nll_loss(f(output), y[split['train']])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            if 'val' in split:\n",
        "                # val split is available\n",
        "                test_acc = evaluator.eval({\n",
        "                    'y_true': y[split['test']].view(-1, 1),\n",
        "                    'y_pred': classifier(z[split['test']]).argmax(-1).view(-1, 1)\n",
        "                })['acc']\n",
        "                val_acc = evaluator.eval({\n",
        "                    'y_true': y[split['val']].view(-1, 1),\n",
        "                    'y_pred': classifier(z[split['val']]).argmax(-1).view(-1, 1)\n",
        "                })['acc']\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_test_acc = test_acc\n",
        "                    best_epoch = epoch\n",
        "            else:\n",
        "                acc = evaluator.eval({\n",
        "                    'y_true': y[split['test']].view(-1, 1),\n",
        "                    'y_pred': classifier(z[split['test']]).argmax(-1).view(-1, 1)\n",
        "                })['acc']\n",
        "                if best_test_acc < acc:\n",
        "                    best_test_acc = acc\n",
        "                    best_epoch = epoch\n",
        "            if verbose:\n",
        "                print(f'logreg epoch {epoch}: best test acc {best_test_acc}')\n",
        "\n",
        "    return {'acc': best_test_acc}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3l7x4hPKkDg"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eLXDVyVI27R"
      },
      "source": [
        "param = {\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"num_hidden\": 256,\n",
        "    \"num_proj_hidden\": 256,\n",
        "    'num_layers': 2,\n",
        "    \"drop_edge_rate_1\": 0.2,\n",
        "    \"drop_edge_rate_2\": 0.3,\n",
        "    \"drop_feature_rate_1\": 0.1,\n",
        "    \"drop_feature_rate_2\": 0.1,\n",
        "    \"tau\": 0.4,\n",
        "    \"num_epochs\": 1000,\n",
        "    'weight_decay': 1e-5,\n",
        "    'drop_scheme': 'degree',\n",
        "}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDmrRy8ZI29j"
      },
      "source": [
        "encoder = Encoder(dataset.num_features, \n",
        "                  param['num_hidden'],\n",
        "                  param['num_layers']).to(device)\n",
        "\n",
        "model = GRACE(encoder, \n",
        "              param['num_hidden'], \n",
        "              param['num_proj_hidden'], \n",
        "              param['tau']).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=param['learning_rate'],\n",
        "        weight_decay=param['weight_decay']\n",
        "    )"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVLhLHHLVZGd"
      },
      "source": [
        "if param['drop_scheme'] == 'degree':\n",
        "    drop_weights = degree_drop_weights(data.edge_index).to(device)\n",
        "\n",
        "    edge_index_ = to_undirected(data.edge_index)\n",
        "    node_deg = degree(edge_index_[1])\n",
        "    feature_weights = feature_drop_weights_dense(data.x, node_c=node_deg).to(device)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3tav3mPai7r"
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    def drop_edge(idx: int):\n",
        "        global drop_weights\n",
        "\n",
        "        return drop_edge_weighted(data.edge_index, \n",
        "                                  drop_weights, \n",
        "                                  p=param[f'drop_edge_rate_{idx}'], \n",
        "                                  threshold=0.7)\n",
        "\n",
        "    edge_index_1 = drop_edge(1)\n",
        "    edge_index_2 = drop_edge(2)\n",
        "    x_1 = drop_feature_weighted_2(data.x, feature_weights, param['drop_feature_rate_1'])\n",
        "    x_2 = drop_feature_weighted_2(data.x, feature_weights, param['drop_feature_rate_2'])\n",
        "\n",
        "    z1 = model(x_1, edge_index_1)\n",
        "    z2 = model(x_2, edge_index_2)\n",
        "\n",
        "    loss = model.loss(z1, z2, None)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIsMc-6WZnI-"
      },
      "source": [
        "def test(final=False):\n",
        "    model.eval()\n",
        "    z = model(data.x, data.edge_index)\n",
        "\n",
        "    evaluator = MulticlassEvaluator()\n",
        "    accs = []\n",
        "    for i in range(10):\n",
        "        acc = log_regression(z, dataset, evaluator, split=split, num_epochs=800)['acc']\n",
        "        accs.append(acc)\n",
        "    acc = sum(accs) / len(accs)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9WC53NpVTgp",
        "outputId": "5ee593d0-7caf-4b91-9d74-20dd9a5be3af"
      },
      "source": [
        "for epoch in range(1, param['num_epochs'] + 1):\n",
        "  loss = train()\n",
        "\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "      acc = test()\n",
        "      print(f'Epoch={epoch:04d}, avg_acc = {acc}, loss={loss:.4f}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch=0100, avg_acc = 0.7366947889328003, loss=95959.8750\n",
            "Epoch=0200, avg_acc = 0.750774472951889, loss=94538.9141\n",
            "Epoch=0300, avg_acc = 0.7531460106372834, loss=93321.4062\n",
            "Epoch=0400, avg_acc = 0.7703129947185516, loss=92597.2891\n",
            "Epoch=0500, avg_acc = 0.7793291091918946, loss=92145.5078\n",
            "Epoch=0600, avg_acc = 0.7836555778980255, loss=91668.0625\n",
            "Epoch=0700, avg_acc = 0.784926813840866, loss=91547.6328\n",
            "Epoch=0800, avg_acc = 0.7875761032104492, loss=91233.2109\n",
            "Epoch=0900, avg_acc = 0.7916461765766144, loss=90988.6953\n",
            "Epoch=1000, avg_acc = 0.7905565500259399, loss=90878.7656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70F-dJdlI3Jg",
        "outputId": "da3484bd-b519-4068-e12e-ab357c243d35"
      },
      "source": [
        " acc = test(final=True)\n",
        "print(f'{acc}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7908449769020081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5k-4eXmoLmT"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}